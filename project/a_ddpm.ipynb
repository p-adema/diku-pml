{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "id": "dD0C2vndcuz6",
    "ExecuteTime": {
     "end_time": "2025-01-07T17:54:19.306909Z",
     "start_time": "2025-01-07T17:54:17.263190Z"
    }
   },
   "source": [
    "from __future__ import annotations\n",
    "from typing import Callable, Any, Literal\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets, transforms, utils\n",
    "from tqdm.auto import tqdm, trange"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "OimlcBLxYkqc",
    "jupyter": {
     "source_hidden": true
    },
    "ExecuteTime": {
     "end_time": "2025-01-07T17:54:19.328672Z",
     "start_time": "2025-01-07T17:54:19.316161Z"
    }
   },
   "source": [
    "# This UNET-style prediction model was originally included as part of the Score-based generative modelling tutorial\n",
    "# by Yang Song et al: https://colab.research.google.com/drive/120kYYBOVa1i0TD85RjlEkFjaWDxSFUx3?usp=sharing\n",
    "\n",
    "\n",
    "class GaussianFourierProjection(nn.Module):\n",
    "    \"\"\"Gaussian random features for encoding time steps.\"\"\"\n",
    "\n",
    "    def __init__(self, embed_dim, scale=30.0):\n",
    "        super().__init__()\n",
    "        # Randomly sample weights during initialization. These weights are fixed\n",
    "        # during optimization and are not trainable.\n",
    "        self.W = nn.Parameter(torch.randn(embed_dim // 2) * scale, requires_grad=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_proj = x[:, None] * self.W[None, :] * 2 * torch.pi\n",
    "        return torch.cat([torch.sin(x_proj), torch.cos(x_proj)], dim=-1)\n",
    "\n",
    "\n",
    "class Dense(nn.Module):\n",
    "    \"\"\"A fully connected layer that reshapes outputs to feature maps.\"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.dense(x)[..., None, None]\n",
    "\n",
    "\n",
    "class ScoreNet(nn.Module):\n",
    "    \"\"\"A time-dependent score-based model built upon U-Net architecture.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        marginal_prob_std: Callable[[torch.Tensor], torch.Tensor] | None,\n",
    "        channels=(32, 64, 128, 256),\n",
    "        embed_dim=256,\n",
    "    ):\n",
    "        \"\"\"Initialize a time-dependent score-based network.\n",
    "\n",
    "        Args:\n",
    "          marginal_prob_std: A function that takes time t and gives the standard\n",
    "            deviation of the perturbation kernel p_{0t}(x(t) | x(0)).\n",
    "          channels: The number of channels for feature maps of each resolution.\n",
    "          embed_dim: The dimensionality of Gaussian random feature embeddings.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # Gaussian random feature embedding layer for time\n",
    "        self.embed = nn.Sequential(\n",
    "            GaussianFourierProjection(embed_dim=embed_dim),\n",
    "            nn.Linear(embed_dim, embed_dim),\n",
    "        )\n",
    "        # Encoding layers where the resolution decreases\n",
    "        self.conv1 = nn.Conv2d(1, channels[0], 3, stride=1, bias=False)\n",
    "        self.dense1 = Dense(embed_dim, channels[0])\n",
    "        self.gnorm1 = nn.GroupNorm(4, num_channels=channels[0])\n",
    "        self.conv2 = nn.Conv2d(channels[0], channels[1], 3, stride=2, bias=False)\n",
    "        self.dense2 = Dense(embed_dim, channels[1])\n",
    "        self.gnorm2 = nn.GroupNorm(32, num_channels=channels[1])\n",
    "        self.conv3 = nn.Conv2d(channels[1], channels[2], 3, stride=2, bias=False)\n",
    "        self.dense3 = Dense(embed_dim, channels[2])\n",
    "        self.gnorm3 = nn.GroupNorm(32, num_channels=channels[2])\n",
    "        self.conv4 = nn.Conv2d(channels[2], channels[3], 3, stride=2, bias=False)\n",
    "        self.dense4 = Dense(embed_dim, channels[3])\n",
    "        self.gnorm4 = nn.GroupNorm(32, num_channels=channels[3])\n",
    "\n",
    "        # Decoding layers where the resolution increases\n",
    "        self.tconv4 = nn.ConvTranspose2d(\n",
    "            channels[3], channels[2], 3, stride=2, bias=False\n",
    "        )\n",
    "        self.dense5 = Dense(embed_dim, channels[2])\n",
    "        self.tgnorm4 = nn.GroupNorm(32, num_channels=channels[2])\n",
    "        self.tconv3 = nn.ConvTranspose2d(\n",
    "            channels[2] + channels[2],\n",
    "            channels[1],\n",
    "            3,\n",
    "            stride=2,\n",
    "            bias=False,\n",
    "            output_padding=1,\n",
    "        )\n",
    "        self.dense6 = Dense(embed_dim, channels[1])\n",
    "        self.tgnorm3 = nn.GroupNorm(32, num_channels=channels[1])\n",
    "        self.tconv2 = nn.ConvTranspose2d(\n",
    "            channels[1] + channels[1],\n",
    "            channels[0],\n",
    "            3,\n",
    "            stride=2,\n",
    "            bias=False,\n",
    "            output_padding=1,\n",
    "        )\n",
    "        self.dense7 = Dense(embed_dim, channels[0])\n",
    "        self.tgnorm2 = nn.GroupNorm(32, num_channels=channels[0])\n",
    "        self.tconv1 = nn.ConvTranspose2d(channels[0] + channels[0], 1, 3, stride=1)\n",
    "\n",
    "        self.act = nn.SiLU()\n",
    "        self.marginal_prob_std = marginal_prob_std\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        # Obtain the Gaussian random feature embedding for t\n",
    "        embed = self.act(self.embed(t))\n",
    "        # Encoding path\n",
    "        h1 = self.conv1(x)\n",
    "        ## Incorporate information from t\n",
    "        h1 += self.dense1(embed)\n",
    "        ## Group normalization\n",
    "        h1 = self.gnorm1(h1)\n",
    "        h1 = self.act(h1)\n",
    "        h2 = self.conv2(h1)\n",
    "        h2 += self.dense2(embed)\n",
    "        h2 = self.gnorm2(h2)\n",
    "        h2 = self.act(h2)\n",
    "        h3 = self.conv3(h2)\n",
    "        h3 += self.dense3(embed)\n",
    "        h3 = self.gnorm3(h3)\n",
    "        h3 = self.act(h3)\n",
    "        h4 = self.conv4(h3)\n",
    "        h4 += self.dense4(embed)\n",
    "        h4 = self.gnorm4(h4)\n",
    "        h4 = self.act(h4)\n",
    "\n",
    "        # Decoding path\n",
    "        h = self.tconv4(h4)\n",
    "        ## Skip connection from the encoding path\n",
    "        h += self.dense5(embed)\n",
    "        h = self.tgnorm4(h)\n",
    "        h = self.act(h)\n",
    "        h = self.tconv3(torch.cat([h, h3], dim=1))\n",
    "        h += self.dense6(embed)\n",
    "        h = self.tgnorm3(h)\n",
    "        h = self.act(h)\n",
    "        h = self.tconv2(torch.cat([h, h2], dim=1))\n",
    "        h += self.dense7(embed)\n",
    "        h = self.tgnorm2(h)\n",
    "        h = self.act(h)\n",
    "        h = self.tconv1(torch.cat([h, h1], dim=1))\n",
    "\n",
    "        # Normalize output (if given a std function)\n",
    "        if self.marginal_prob_std is not None:\n",
    "            h /= self.marginal_prob_std(t)[:, None, None, None]\n",
    "        return h"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "id": "DSEwkf9kcuz-",
    "ExecuteTime": {
     "end_time": "2025-01-07T17:54:19.470003Z",
     "start_time": "2025-01-07T17:54:19.467067Z"
    }
   },
   "source": [
    "# ExponentialMovingAverage implementation as used in pytorch vision\n",
    "# https://github.com/pytorch/vision/blob/main/references/classification/utils.py#L159\n",
    "\n",
    "# BSD 3-Clause License\n",
    "\n",
    "# Copyright (c) Soumith Chintala 2016,\n",
    "# All rights reserved.\n",
    "\n",
    "# Redistribution and use in source and binary forms, with or without\n",
    "# modification, are permitted provided that the following conditions are met:\n",
    "\n",
    "# * Redistributions of source code must retain the above copyright notice, this\n",
    "#   list of conditions and the following disclaimer.\n",
    "\n",
    "# * Redistributions in binary form must reproduce the above copyright notice,\n",
    "#   this list of conditions and the following disclaimer in the documentation\n",
    "#   and/or other materials provided with the distribution.\n",
    "\n",
    "# * Neither the name of the copyright holder nor the names of its\n",
    "#   contributors may be used to endorse or promote products derived from\n",
    "#   this software without specific prior written permission.\n",
    "\n",
    "# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\n",
    "# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n",
    "# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\n",
    "# DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE\n",
    "# FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\n",
    "# DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\n",
    "# SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\n",
    "# CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,\n",
    "# OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n",
    "# OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n",
    "\n",
    "\n",
    "class ExponentialMovingAverage(torch.optim.swa_utils.AveragedModel):\n",
    "    \"\"\"Maintains moving averages of model parameters using an exponential decay.\n",
    "    ``ema_avg = decay * avg_model_param + (1 - decay) * model_param``\n",
    "    `torch.optim.swa_utils.AveragedModel <https://pytorch.org/docs/stable/optim.html#custom-averaging-strategies>`_\n",
    "    is used to compute the EMA.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, avg_model, decay, avg_device=\"cpu\"):\n",
    "        def ema_avg(avg_model_param, model_param, _num_averaged):\n",
    "            return decay * avg_model_param + (1 - decay) * model_param\n",
    "\n",
    "        super().__init__(avg_model, avg_device, ema_avg, use_buffers=True)"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Jvaeyy-3cuz-",
    "ExecuteTime": {
     "end_time": "2025-01-07T17:54:19.496378Z",
     "start_time": "2025-01-07T17:54:19.482509Z"
    }
   },
   "source": [
    "class DDPM(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        network,\n",
    "        max_t: int = 100,\n",
    "        beta_1: float = 1e-4,\n",
    "        beta_max_t: float = 2e-2,\n",
    "        predict_mean_by: Literal[\"e\", \"u\", \"x0\"] = \"e\",\n",
    "        reduce_variance_by: Literal[\"low-discrepency\", \"importance-sampling\"]\n",
    "        | None = None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize Denoising Diffusion Probabilistic Model\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        network: nn.Module\n",
    "            The inner neural network used by the diffusion process. Typically a Unet.\n",
    "        beta_1: float\n",
    "            beta_t value at t=1\n",
    "        beta_max_t: [float]\n",
    "            beta_t value at t=T (last step)\n",
    "        max_t: int\n",
    "            The number of diffusion steps.\n",
    "        predict_mean_by: str\n",
    "            Which target (epsilon, x_t-1 mean, x_0 mean) the network should predict.\n",
    "        reduce_variance_by: str | None\n",
    "            An optional method to reduce the variance of the loss estimate.\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        # Normalize time input before evaluating neural network\n",
    "        # Reshape input into image format and normalize time value before sending it to network model\n",
    "        self._network = network\n",
    "\n",
    "        # Total number of time steps\n",
    "        self.max_t = max_t\n",
    "        self.predict_mean_by = predict_mean_by\n",
    "        self.reduce_variance_by = reduce_variance_by\n",
    "\n",
    "        # Registering as buffers to ensure they get transferred to the GPU automatically\n",
    "        self.register_buffer(\"beta\", torch.linspace(beta_1, beta_max_t, max_t + 1))\n",
    "        self.register_buffer(\"alpha\", 1 - self.beta)\n",
    "        self.register_buffer(\"alpha_bar\", self.alpha.cumprod(dim=0))\n",
    "\n",
    "        if self.reduce_variance_by == \"importance-sampling\":\n",
    "            self.imps_warmup = True\n",
    "            history_length = 10\n",
    "            self.register_buffer(\n",
    "                \"imps_idx\", torch.zeros((self.max_t + 1,), dtype=torch.int64)\n",
    "            )\n",
    "            self.register_buffer(\n",
    "                \"imps_hist\", torch.zeros((self.max_t + 1, history_length))\n",
    "            )\n",
    "            startup_ts = (\n",
    "                torch.arange(1, self.max_t + 1)\n",
    "                .expand((history_length, max_t))\n",
    "                .flatten()\n",
    "            )\n",
    "            self.register_buffer(\n",
    "                \"imps_startup_ts\", startup_ts[torch.randperm(startup_ts.numel())]\n",
    "            )\n",
    "            self.imps_startup_idx = 0\n",
    "\n",
    "    def network(self, x, t):\n",
    "        return self._network(\n",
    "            x.reshape(-1, 1, 28, 28), t.squeeze() / self.max_t\n",
    "        ).reshape(-1, 28 * 28)\n",
    "\n",
    "    def forward_diffusion(self, x0, t, epsilon):\n",
    "        \"\"\"\n",
    "        q(x_t | x_0)\n",
    "        Forward diffusion from an input datapoint x0 to an xt at timestep t, provided a N(0,1) noise sample epsilon.\n",
    "        Note that we can do this operation in a single step\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x0: torch.tensor\n",
    "            x value at t=0 (an input image)\n",
    "        t: int\n",
    "            step index\n",
    "        epsilon:\n",
    "            noise sample\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.tensor\n",
    "            image at timestep t\n",
    "        \"\"\"\n",
    "\n",
    "        mean = torch.sqrt(self.alpha_bar[t]) * x0\n",
    "        std = torch.sqrt(1 - self.alpha_bar[t])\n",
    "\n",
    "        return mean + std * epsilon\n",
    "\n",
    "    def reverse_diffusion(self, xt, t, epsilon):\n",
    "        \"\"\"\n",
    "        p(x_{t-1} | x_t)\n",
    "        Single step in the reverse direction, from x_t (at timestep t) to x_{t-1}, provided a N(0,1) noise sample epsilon.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        xt: torch.tensor\n",
    "            x value at step t\n",
    "        t: int\n",
    "            step index\n",
    "        epsilon:\n",
    "            noise sample\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.tensor\n",
    "            image at timestep t-1\n",
    "        \"\"\"\n",
    "        if self.predict_mean_by == \"e\":\n",
    "            mean = (\n",
    "                1.0\n",
    "                / torch.sqrt(self.alpha[t])\n",
    "                * (\n",
    "                    xt\n",
    "                    - (self.beta[t])\n",
    "                    / torch.sqrt(1 - self.alpha_bar[t])\n",
    "                    * self.network(xt, t)\n",
    "                )\n",
    "            )\n",
    "        elif self.predict_mean_by == \"u\":\n",
    "            mean = self.network(xt, t)\n",
    "        elif self.predict_mean_by == \"x0\":\n",
    "            full_epsilon = xt - self.network(xt, t)\n",
    "            mean = (1.0 / torch.sqrt(1 - self.beta[t])) * (\n",
    "                xt - (self.beta[t] / torch.sqrt(1 - self.alpha_bar[t])) * full_epsilon\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid value: {self.predict_mean_by=}\")\n",
    "\n",
    "        mask = t > 0\n",
    "        std = torch.where(\n",
    "            mask,\n",
    "            torch.sqrt(\n",
    "                ((1 - self.alpha_bar[t - 1]) / (1 - self.alpha_bar[t])) * self.beta[t]\n",
    "            ),\n",
    "            0,\n",
    "        )\n",
    "\n",
    "        return mean + std * epsilon\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def sample(self, shape):\n",
    "        \"\"\"\n",
    "        Sample from diffusion model (Algorithm 2 in Ho et al, 2020)\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        shape: tuple\n",
    "            Specify shape of sampled output. For MNIST: (nsamples, 28*28)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.tensor\n",
    "            sampled image\n",
    "        \"\"\"\n",
    "\n",
    "        # Sample xT: Gaussian noise\n",
    "        xt = torch.randn(shape).to(self.beta.device)\n",
    "        for t in range(self.max_t, 0, -1):\n",
    "            noise = torch.randn_like(xt) if t > 1 else 0\n",
    "            t = torch.tensor(t).expand(xt.shape[0], 1).to(self.beta.device)\n",
    "            xt = self.reverse_diffusion(xt, t, noise)\n",
    "\n",
    "        return xt\n",
    "\n",
    "    def elbo_simple(self, x0: torch.Tensor):\n",
    "        \"\"\"\n",
    "        ELBO training objective (Algorithm 1 in Ho et al, 2020)\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x0: torch.tensor\n",
    "            Input image\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        float\n",
    "            ELBO value\n",
    "        \"\"\"\n",
    "        # Sample time step t\n",
    "        b_size = x0.shape[0]\n",
    "        if self.reduce_variance_by is None:\n",
    "            t = torch.randint(1, self.max_t, (b_size, 1)).to(x0.device)\n",
    "            t_weights = torch.ones_like(t)\n",
    "        elif self.reduce_variance_by == \"low-discrepency\":\n",
    "            # Remove last, since it's equal under modulus. Add extra one to compensate rounding at start and end\n",
    "            ts_base = torch.linspace(1, self.max_t + 1, b_size + 1)[:-1]\n",
    "            # Random addition between 0 and the step size of ts_base\n",
    "            ts_offset = torch.rand_like(ts_base) * ts_base[1]\n",
    "            # Taking a t value from a random index, yields a marginal distribution that is uniform over the reals [1, max_t+1).\n",
    "            # Taking the floor, yields a marginal distribution that is unfirom over the integers [1, max_t]\n",
    "            # Clamp to ensure floating point errors don't cause out-of-bounds\n",
    "            # Unlike \"Variational Diffusion Models\", individual positions (e.g. the first t) are not uniformly distributed,\n",
    "            # this could be sovled with a random permutation, but that is needless work.\n",
    "            t = (\n",
    "                (ts_base + ts_offset)\n",
    "                .floor()\n",
    "                .int()\n",
    "                .clamp(1, self.max_t)\n",
    "                .view((b_size, 1))\n",
    "                .to(x0.device)\n",
    "            )\n",
    "            t_weights = torch.ones_like(t)\n",
    "        elif self.reduce_variance_by == \"importance-sampling\":\n",
    "            if (\n",
    "                self.imps_warmup\n",
    "                and self.imps_startup_idx + b_size < self.imps_startup_ts.numel()\n",
    "            ):\n",
    "                t = (\n",
    "                    self.imps_startup_ts[\n",
    "                        self.imps_startup_idx : self.imps_startup_idx + b_size\n",
    "                    ]\n",
    "                    .view((b_size, 1))\n",
    "                    .to(x0.device)\n",
    "                )\n",
    "                t_weights = torch.ones_like(t)\n",
    "                self.imps_startup_idx += b_size\n",
    "            elif self.imps_warmup:\n",
    "                self.imps_warmup = False\n",
    "                print(f\"Warmup complete, just-before counts\\n{self.imps_idx}\")\n",
    "                missing_warmup = (\n",
    "                    self.imps_startup_idx + b_size - self.imps_startup_ts.numel()\n",
    "                )\n",
    "                missing_elems = torch.randint(1, self.max_t, (missing_warmup,)).to(x0.device)\n",
    "                t = torch.cat(\n",
    "                    (self.imps_startup_ts[self.imps_startup_idx :], missing_elems),\n",
    "                    dim=0,\n",
    "                ).view((b_size, 1))\n",
    "                t_weights = torch.ones_like(t)\n",
    "            else:\n",
    "                t = torch.randint(1, self.max_t, (b_size, 1)).to(x0.device)\n",
    "                weights = torch.sqrt(self.imps_hist.mean(dim=1)).to(x0.device)\n",
    "                t_weights = weights[t.view(-1)].view_as(t)\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid value: {self.reduce_variance_by=}\")\n",
    "\n",
    "        # Sample noise\n",
    "        epsilon = torch.randn_like(x0)\n",
    "\n",
    "        xt = self.forward_diffusion(x0, t, epsilon)\n",
    "\n",
    "        if self.predict_mean_by == \"e\":\n",
    "            target = epsilon\n",
    "        elif self.predict_mean_by == \"u\":\n",
    "            target = self.forward_diffusion(x0, t - 1, epsilon)\n",
    "        elif self.predict_mean_by == \"x0\":\n",
    "            target = x0\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid value: {self.predict_mean_by=}\")\n",
    "\n",
    "        unweighted_se = nn.MSELoss(reduction=\"none\")(target, self.network(xt, t)).mean(\n",
    "            tuple(range(1, len(target.shape)))\n",
    "        )\n",
    "        assert unweighted_se.shape == (\n",
    "            b_size,\n",
    "        ), f\"Strange shape {unweighted_se.shape=} {x0.shape=}\"\n",
    "\n",
    "        if self.reduce_variance_by == \"importance-sampling\":\n",
    "            uniq_t, rev_t, count_t = torch.unique(\n",
    "                t.view(-1), return_inverse=True, return_counts=True\n",
    "            )\n",
    "            assert rev_t.shape == (b_size,), f\"Strange shape {rev_t=}\"\n",
    "            # Nth occurence (zero-indexed) of the particular value of t\n",
    "            same_t = torch.eq(rev_t.view((-1, 1)), rev_t.view((1, -1)))\n",
    "            nth_occurence = torch.triu(same_t).sum(dim=0) - 1\n",
    "            assert nth_occurence.shape == (b_size,), f\"Strange shape {nth_occurence=}\"\n",
    "            hist_idx = (self.imps_idx[t.view(-1)] + nth_occurence) % self.imps_hist.shape[1]\n",
    "            assert (\n",
    "                len(self.imps_hist.shape) == 2\n",
    "            ), f\"Strange shape {self.imps_hist.shape=}\"\n",
    "            assert hist_idx.shape == (b_size,), f\"Strange shape {hist_idx.shape=}\"\n",
    "            self.imps_hist[t.view(-1), hist_idx] = unweighted_se.detach()**2\n",
    "            self.imps_idx[uniq_t] += count_t\n",
    "\n",
    "        return -(unweighted_se * t_weights / t_weights.sum()).mean()\n",
    "\n",
    "    def loss(self, x0):\n",
    "        \"\"\"\n",
    "        Loss function. Just the negative of the ELBO.\n",
    "        \"\"\"\n",
    "        return -self.elbo_simple(x0)"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "By7MSuvVcuz_",
    "ExecuteTime": {
     "end_time": "2025-01-07T17:54:19.524933Z",
     "start_time": "2025-01-07T17:54:19.520346Z"
    }
   },
   "source": [
    "def train(\n",
    "    train_model: nn.Module,\n",
    "    train_optimizer: torch.optim.Optimizer,\n",
    "    train_scheduler: torch.optim.lr_scheduler.LRScheduler,\n",
    "    train_dataloader: torch.utils.data.DataLoader[tuple[torch.Tensor, Any]],\n",
    "    train_epochs: int,\n",
    "    train_device: torch.device,\n",
    "    ema: bool = True,\n",
    "    per_epoch_callback: Callable = None,\n",
    "    double_bar: bool = False,\n",
    "    extra_desc: str = None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Training loop\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    train_model: nn.Module\n",
    "        Pytorch model\n",
    "    train_optimizer: optim.Optimizer\n",
    "        Pytorch optimizer to be used for training\n",
    "    train_scheduler: optim.LRScheduler\n",
    "        Pytorch learning rate scheduler\n",
    "    train_dataloader: utils.DataLoader\n",
    "        Pytorch dataloader\n",
    "    train_epochs: int\n",
    "        Number of epochs to train\n",
    "    train_device: torch.device\n",
    "        Pytorch device specification\n",
    "    ema: Boolean\n",
    "        Whether to activate Exponential Model Averaging\n",
    "    per_epoch_callback: function\n",
    "        Called at the end of every epoch\n",
    "    double_bar: bool\n",
    "        Whether to add a progress bar within an epoch\n",
    "    extra_desc: str\n",
    "        Extra progress bar description\n",
    "    \"\"\"\n",
    "\n",
    "    # Setup progress bar\n",
    "\n",
    "    if ema:\n",
    "        ema_global_step_counter = 0\n",
    "        ema_steps = 10\n",
    "        ema_adjust = train_dataloader.batch_size * ema_steps / train_epochs\n",
    "        ema_decay = 1.0 - 0.995\n",
    "        ema_alpha = min(1.0, (1.0 - ema_decay) * ema_adjust)\n",
    "        ema_model = ExponentialMovingAverage(\n",
    "            train_model, avg_device=train_device, decay=1.0 - ema_alpha\n",
    "        )\n",
    "\n",
    "    full_bar = trange(\n",
    "        train_epochs,\n",
    "        desc=\"Training\" if not extra_desc else f\"Training ({extra_desc})\",\n",
    "        smoothing=0.05,\n",
    "        unit=\"epoch\",\n",
    "        position=1,\n",
    "    )\n",
    "    for epoch in full_bar:\n",
    "        # Switch to train mode\n",
    "        train_model.train()\n",
    "\n",
    "        epoch_bar = tqdm(\n",
    "            train_dataloader,\n",
    "            leave=False,\n",
    "            position=2,\n",
    "            desc=f\"Epoch {epoch}\",\n",
    "            disable=not double_bar,\n",
    "        )\n",
    "        for x, _label in epoch_bar:\n",
    "            x = x.to(train_device)\n",
    "            train_optimizer.zero_grad()\n",
    "            loss = train_model.loss(x)\n",
    "            loss.backward()\n",
    "            train_optimizer.step()\n",
    "            train_scheduler.step()\n",
    "\n",
    "            # Update progress bar\n",
    "            epoch_bar.set_postfix(\n",
    "                loss=f\"⠀{loss.item():12.4f}\",\n",
    "                lr=f\"{train_scheduler.get_last_lr()[0]:.2E}\",\n",
    "                refresh=False,\n",
    "            )\n",
    "\n",
    "            if ema:\n",
    "                ema_global_step_counter += 1\n",
    "                if ema_global_step_counter % ema_steps == 0:\n",
    "                    ema_model.update_parameters(train_model)\n",
    "\n",
    "        if per_epoch_callback:\n",
    "            per_epoch_callback(ema_model.module if ema else train_model)"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "gqPZ6PkScuz_",
    "ExecuteTime": {
     "end_time": "2025-01-07T17:54:19.542020Z",
     "start_time": "2025-01-07T17:54:19.540293Z"
    }
   },
   "source": [
    "# Parameters\n",
    "T = 1000\n",
    "learning_rate = 1e-3\n",
    "epochs = 100\n",
    "batch_size = 256"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "tBntJ5JFcuz_",
    "ExecuteTime": {
     "end_time": "2025-01-07T17:54:19.598771Z",
     "start_time": "2025-01-07T17:54:19.548487Z"
    }
   },
   "source": [
    "# Rather than treating MNIST images as discrete objects, as done in Ho et al 2020,\n",
    "# we here treat them as continuous input data, by dequantizing the pixel values (adding noise to the input data)\n",
    "# Also note that we map the 0..255 pixel values to [-1, 1], and that we process the 28x28 pixel values as a flattened 784 tensor.\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Lambda(\n",
    "            lambda x: x + torch.rand(x.shape) / 255\n",
    "        ),  # Dequantize pixel values\n",
    "        transforms.Lambda(lambda x: (x - 0.5) * 2.0),  # Map from [0,1] -> [-1, -1]\n",
    "        transforms.Lambda(lambda x: x.flatten()),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Download and transform train dataset\n",
    "dataloader_train = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST(\"./mnist_data\", download=True, train=True, transform=transform),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aYJ6BwrGcuz_",
    "outputId": "d0b19ba1-efa3-418a-8d93-2ba69a0d3cbb",
    "ExecuteTime": {
     "end_time": "2025-01-07T17:54:19.621306Z",
     "start_time": "2025-01-07T17:54:19.602320Z"
    }
   },
   "source": [
    "# Select device\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda:0\"\n",
    "elif torch.mps.is_available():\n",
    "    device = \"mps:0\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "print(f\"Running on {device}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on mps:0\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "KLr9MxVncu0A",
    "ExecuteTime": {
     "end_time": "2025-01-07T17:54:19.630370Z",
     "start_time": "2025-01-07T17:54:19.627561Z"
    }
   },
   "source": [
    "def reporter(train_model: nn.Module):\n",
    "    \"\"\"Callback function used for plotting images during training\"\"\"\n",
    "\n",
    "    # Switch to eval mode\n",
    "    train_model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        nsamples = 10\n",
    "        samples = train_model.sample((nsamples, 28 * 28)).cpu()\n",
    "\n",
    "        # Map pixel values back from [-1,1] to [0,1]\n",
    "        samples = (samples + 1) / 2\n",
    "        samples = samples.clamp(0.0, 1.0)\n",
    "\n",
    "        # Plot in grid\n",
    "        grid = utils.make_grid(samples.reshape(-1, 1, 28, 28), nrow=nsamples)\n",
    "        plt.gca().set_axis_off()\n",
    "        plt.imshow(transforms.functional.to_pil_image(grid), cmap=\"gray\")\n",
    "        plt.show()"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mcoxR2ajYkqe",
    "outputId": "96a34758-3d41-4f41-db59-40d7f87fc36d",
    "ExecuteTime": {
     "end_time": "2025-01-07T17:54:19.649361Z",
     "start_time": "2025-01-07T17:54:19.646139Z"
    }
   },
   "source": [
    "# Base models, three different ways to predict the mean\n",
    "for tgt in (\"e\", \"u\", \"x0\"):\n",
    "    if Path(f\"ddpm_models/base_{tgt}.pt\").exists():\n",
    "        print(f\"Skipping target {tgt}, already exists\")\n",
    "        continue\n",
    "\n",
    "    torch.manual_seed(0)\n",
    "\n",
    "    # Construct Unet\n",
    "    # The original ScoreNet expects a function with std for all the\n",
    "    # different noise levels, such that the output can be rescaled.\n",
    "    # Since we are predicting the noise (rather than the score), we\n",
    "    # ignore this rescaling and just set std=1 for all t.\n",
    "    mnist_unet = ScoreNet(None)\n",
    "\n",
    "    # Construct model\n",
    "    model = DDPM(mnist_unet, max_t=T, predict_mean_by=tgt).to(device)\n",
    "    model.compile(fullgraph=True)\n",
    "\n",
    "    # Construct optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Setup simple scheduler\n",
    "    scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, 0.9999)\n",
    "\n",
    "    # Call training loop\n",
    "    train(\n",
    "        model,\n",
    "        optimizer,\n",
    "        scheduler,\n",
    "        dataloader_train,\n",
    "        train_epochs=epochs,\n",
    "        train_device=device,\n",
    "        ema=True,\n",
    "        per_epoch_callback=reporter,\n",
    "        extra_desc=f\"target={tgt}\",\n",
    "    )\n",
    "\n",
    "    torch.save(model, f\"ddpm_models/base_{tgt}.pt\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping target e, already exists\n",
      "Skipping target u, already exists\n",
      "Skipping target x0, already exists\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "e4a57c21a48d481ab5bccb1cc2b2fb94",
      "ad4cf554826a4657b08729043b80ca85",
      "dc518a61d09b4bc09bf7b83bef7bf509",
      "239b4f5f294944d6b9301e6bae6c4ce3",
      "39e00ddf568e4f669f2e058787577b94",
      "8f90390a02554e57b6327fedb672ae37",
      "41380c33122743e0b9280eecad619369",
      "b1a2e39c08654a9e8121676a065bedb4",
      "47217f6771d24f4d9f3185fcff4d0645",
      "f41c09d80e5b4a15a37bc4f31e1a1b8f",
      "4785b949f00d48dd999aae4363359b01",
      "72afeda3fef749df9d125111d82ef5ca",
      "b93901205b40486491003f03995bd4cc",
      "526dffa9e023446d8cd3497344e2d9a7",
      "a7eec86f879949d3bf2d8bbc7c81c145",
      "acc689abcda0425297e094842de1c429",
      "1fe3d7beb25b4972b0a978bb062c7cac",
      "14aefff1d0524f628d52da1a2c7854e0",
      "4d9d50ae00124553a7ce7c67c6471e83",
      "3a335100ce034c71995e78ab709c12d8",
      "4bda0b249671454e86137da6851324b4",
      "eae940680eac4b84af0db1b16080f55c"
     ]
    },
    "id": "QjNqE_6Pcu0A",
    "outputId": "0e9fc720-6ba7-41fd-ef6d-c57b4393e376",
    "ExecuteTime": {
     "end_time": "2025-01-07T17:54:19.660897Z",
     "start_time": "2025-01-07T17:54:19.656607Z"
    }
   },
   "source": [
    "for mthd in (\"low-discrepency\", \"importance-sampling\"):\n",
    "    if Path(f\"ddpm_models/variance_{mthd}.pt\").exists():\n",
    "        print(f\"Skipping method {mthd}, already exists\")\n",
    "        continue\n",
    "\n",
    "    torch.manual_seed(0)\n",
    "\n",
    "    # Construct Unet\n",
    "    # The original ScoreNet expects a function with std for all the\n",
    "    # different noise levels, such that the output can be rescaled.\n",
    "    # Since we are predicting the noise (rather than the score), we\n",
    "    # ignore this rescaling and just set std=1 for all t.\n",
    "    mnist_unet = ScoreNet(None)\n",
    "\n",
    "    # Construct model\n",
    "    model = DDPM(mnist_unet, max_t=T, reduce_variance_by=mthd).to(device)\n",
    "    model.compile(fullgraph=True)\n",
    "\n",
    "    # Construct optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Setup simple scheduler\n",
    "    scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, 0.9999)\n",
    "\n",
    "    # Call training loop\n",
    "    train(\n",
    "        model,\n",
    "        optimizer,\n",
    "        scheduler,\n",
    "        dataloader_train,\n",
    "        train_epochs=epochs,\n",
    "        train_device=device,\n",
    "        ema=True,\n",
    "        per_epoch_callback=reporter,\n",
    "        extra_desc=f\"method={mthd}\",\n",
    "    )\n",
    "\n",
    "    torch.save(model, f\"ddpm_models/variance_{mthd}.pt\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping method low-discrepency, already exists\n",
      "Skipping method importance-sampling, already exists\n"
     ]
    }
   ],
   "execution_count": 11
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "e4a57c21a48d481ab5bccb1cc2b2fb94": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_ad4cf554826a4657b08729043b80ca85",
       "IPY_MODEL_dc518a61d09b4bc09bf7b83bef7bf509",
       "IPY_MODEL_239b4f5f294944d6b9301e6bae6c4ce3"
      ],
      "layout": "IPY_MODEL_39e00ddf568e4f669f2e058787577b94"
     }
    },
    "ad4cf554826a4657b08729043b80ca85": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8f90390a02554e57b6327fedb672ae37",
      "placeholder": "​",
      "style": "IPY_MODEL_41380c33122743e0b9280eecad619369",
      "value": "Training (method=low-discrepency): 100%"
     }
    },
    "dc518a61d09b4bc09bf7b83bef7bf509": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b1a2e39c08654a9e8121676a065bedb4",
      "max": 100,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_47217f6771d24f4d9f3185fcff4d0645",
      "value": 100
     }
    },
    "239b4f5f294944d6b9301e6bae6c4ce3": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f41c09d80e5b4a15a37bc4f31e1a1b8f",
      "placeholder": "​",
      "style": "IPY_MODEL_4785b949f00d48dd999aae4363359b01",
      "value": " 100/100 [34:21&lt;00:00, 20.43s/epoch]"
     }
    },
    "39e00ddf568e4f669f2e058787577b94": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8f90390a02554e57b6327fedb672ae37": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "41380c33122743e0b9280eecad619369": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b1a2e39c08654a9e8121676a065bedb4": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "47217f6771d24f4d9f3185fcff4d0645": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "f41c09d80e5b4a15a37bc4f31e1a1b8f": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4785b949f00d48dd999aae4363359b01": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "72afeda3fef749df9d125111d82ef5ca": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_b93901205b40486491003f03995bd4cc",
       "IPY_MODEL_526dffa9e023446d8cd3497344e2d9a7",
       "IPY_MODEL_a7eec86f879949d3bf2d8bbc7c81c145"
      ],
      "layout": "IPY_MODEL_acc689abcda0425297e094842de1c429"
     }
    },
    "b93901205b40486491003f03995bd4cc": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1fe3d7beb25b4972b0a978bb062c7cac",
      "placeholder": "​",
      "style": "IPY_MODEL_14aefff1d0524f628d52da1a2c7854e0",
      "value": "Training (method=importance-sampling): 100%"
     }
    },
    "526dffa9e023446d8cd3497344e2d9a7": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4d9d50ae00124553a7ce7c67c6471e83",
      "max": 100,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_3a335100ce034c71995e78ab709c12d8",
      "value": 100
     }
    },
    "a7eec86f879949d3bf2d8bbc7c81c145": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4bda0b249671454e86137da6851324b4",
      "placeholder": "​",
      "style": "IPY_MODEL_eae940680eac4b84af0db1b16080f55c",
      "value": " 100/100 [34:12&lt;00:00, 20.32s/epoch]"
     }
    },
    "acc689abcda0425297e094842de1c429": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1fe3d7beb25b4972b0a978bb062c7cac": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "14aefff1d0524f628d52da1a2c7854e0": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4d9d50ae00124553a7ce7c67c6471e83": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3a335100ce034c71995e78ab709c12d8": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "4bda0b249671454e86137da6851324b4": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "eae940680eac4b84af0db1b16080f55c": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
